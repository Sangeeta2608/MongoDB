{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.What are the key differences between SQL and NoSQL databases.\n",
        "SQL databases (also known as Relational Databases) use a structured data model based on tables, with rows and columns representing records and fields respectively. Each table has a predefined schema that enforces data types and constraints.\n",
        "SQL databases use Structured Query Language (SQL), which provides powerful and standardized commands for querying, updating, and managing relational data.\n",
        "SQL is ideal for applications requiring structured data, complex queries, and strict consistency, such as financial systems, customer relationship management (CRM), and enterprise resource planning (ERP).\n",
        "\n",
        "NoSQL databases (Non-relational Databases) use a flexible data model that can be document-based, key-value pairs, wide-column stores, or graph-based. These databases often allow schema-less data storage, meaning different records in the same collection can have different fields.\n",
        "NoSQL databases use varied query languages, depending on the type of database. For example:\n",
        "MongoDB uses a JSON-like query language,\n",
        "Cassandra uses CQL (Cassandra Query Language),\n",
        "Redis uses simple commands for key-value access.\n",
        "NoSQL is better suited for large-scale data storage, real-time web applications, and rapid development environments, such as social media platforms, IoT applications, and big data analytics.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZzPstbrspzqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.What makes MongoDB a good choice for modern applications.\n",
        "MongoDB is a popular NoSQL document-oriented database, and it's widely regarded as a strong choice for modern applications due to several key features and architectural advantages.MongoDB is well-suited for modern, dynamic, cloud-based, and data-driven applications that demand scalability, performance, and flexibility. It empowers developers to build and iterate quickly while handling large and complex data efficiently."
      ],
      "metadata": {
        "id": "MQOCO8r3q1uF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Explain the concept of collections in MongoDB.\n",
        "A collection in MongoDB is a flexible, schema-less container for documents. It allows developers to store varied and evolving data structures in a scalable and efficient way, making it ideal for modern applications with dynamic requirements.In MongoDB, a collection is the equivalent of a table in relational databases. It is a group of related documents stored together in the same database.\n",
        "\n"
      ],
      "metadata": {
        "id": "kCbOm15Jte2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.How does MongoDB ensure high availability using replication.\n",
        "MongoDB ensures high availability through a feature called a Replica Set.\n",
        "A replica set in MongoDB is a group of MongoDB servers (instances) that maintain the same data set.\n",
        "It provides redundancy and automatic failover, ensuring that the database remains available even if one or more servers fail.\n",
        "MongoDB achieves high availability through replica sets, which ensure that your application can recover quickly from failures without manual intervention. This makes MongoDB well-suited for mission-critical, distributed, and cloud-based applications."
      ],
      "metadata": {
        "id": "VlkxHhhGuDDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.What are the main benefits of MongoDB Atlas?\n",
        "MongoDB Atlas is the fully managed cloud version of MongoDB, offered as a Database-as-a-Service (DBaaS) on platforms like AWS, Google Cloud, and Azure.\n",
        "It removes much of the operational complexity and adds powerful features for modern, scalable applications.MongoDB Atlas simplifies deployment, ensures reliability, and provides enterprise-grade features for building, running, and scaling modern applications in the cloud — with minimal operational overhead."
      ],
      "metadata": {
        "id": "AmRc8sgtufI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.What is the role of indexes in MongoDB, and how do they improve performance.\n",
        "An index in MongoDB is a special data structure that stores a subset of fields from documents in a collection, in a sorted order.\n",
        "Just like indexes in a book help you find a topic quickly, MongoDB indexes help the database engine locate data faster without scanning every document.\n",
        "Indexes improve performance by\n",
        "1.Speed Up Query Execution\n",
        "Without an index, MongoDB performs a collection scan, checking every document.\n",
        "With an index, MongoDB quickly narrows down the results, drastically reducing search time.\n",
        "2.Optimize Sorting Operations\n",
        "Indexes maintain documents in a sorted order, so sort() queries are much faster when sorted fields are indexed.\n",
        "3.Improve Query Selectivity\n",
        "Indexes allow efficient execution of queries with conditions like $eq, $gt, $lt, $in, etc.\n",
        "They help avoid loading unnecessary documents into memory.\n",
        "4.Support Uniqueness Constraints\n",
        "MongoDB uses unique indexes to enforce that a field (like email or username) contains no duplicate values.\n",
        "5.Enable Efficient Joins and Lookups\n",
        "When using $lookup for joins or foreign keys, indexing the referenced field improves performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "4u2Qo3pJvATd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.Describe the stages of the MongoDB aggregation pipeline.\n",
        "The aggregation pipeline in MongoDB is a framework for processing data in a series of stages, where each stage transforms the documents and passes the results to the next stage. It's used for data transformation, analytics, and reporting.\n",
        "Overview of the Aggregation Pipeline\n",
        "Similar to a data processing pipeline.\n",
        "Input: Documents from a collection.\n",
        "Output: Transformed results (e.g., totals, averages, grouped data).\n",
        "The MongoDB aggregation pipeline is a powerful tool for transforming, analyzing, and summarizing data. By chaining stages like $match, $group, $project, and others, you can build complex queries for reporting, analytics, or dashboards — all inside MongoDB.\n",
        "\n"
      ],
      "metadata": {
        "id": "l7W5uIGGwaqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8.What is sharding in MongoDB? How does it differ from replication.\n",
        "**Sharding** is the process of distributing data across multiple machines (or shards) to support horizontal scaling in MongoDB. It's used to handle large datasets and high throughput workloads by breaking data into smaller, more manageable parts.\n",
        "How Sharding Works\n",
        "The shard key divides data into chunks.\n",
        "Each chunk is assigned to a shard.\n",
        "The mongos router sends queries to the appropriate shard(s).\n",
        "Data can be balanced across shards automatically based on load or size.\n",
        "**Replication** is the process of copying data from one MongoDB server to others to ensure data redundancy and high availability.\n",
        "Implemented via replica sets, where:\n",
        "One node is primary (handles writes).\n",
        "Sharding helps MongoDB scale out to handle large and growing data sets by distributing data.\n",
        "Replication ensures that MongoDB remains reliable and available by copying data across multiple servers.Others are secondaries (read-only copies, can take over if primary fails).\n",
        "\n"
      ],
      "metadata": {
        "id": "Q5Q-_YVIxBl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9.What is PyMongo, and why is it used?\n",
        "PyMongo is the official Python driver for MongoDB.\n",
        "It is a Python library that allows developers to interact with MongoDB databases from within Python application.\n",
        "PyMongo is used to:\n",
        "Connect to MongoDB from a Python application.\n",
        "Perform CRUD operations (Create, Read, Update, Delete).\n",
        "Execute queries, including aggregation pipelines and indexing.\n",
        "Manage databases and collections.\n",
        "Integrate MongoDB functionality into data-driven Python applications, such as web apps, analytics tools, or data pipelines."
      ],
      "metadata": {
        "id": "-UabC8l5xq_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10.What are the ACID properties in the context of MongoDB transactions?\n",
        "In database systems, ACID is an acronym that stands for:Atomicity,Consistency,Isolation,Durability.\n",
        "These properties ensure the reliability and integrity of data during a transaction.\n",
        "MongoDB's support for ACID transactions makes it suitable for mission-critical applications that require strict data integrity, such as:Financial systems,Order management,Inventory tracking.\n",
        "While MongoDB is traditionally schema-flexible and performance-oriented, its ability to handle ACID-compliant transactions bridges the gap with traditional relational databases for complex and reliable operations."
      ],
      "metadata": {
        "id": "k--hlsrdx7LP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11. What is the purpose of MongoDB’s explain() function.\n",
        "The explain() function in MongoDB is used to analyze and understand how a query is executed by the database engine. It provides detailed information about the query plan, which helps developers and DBAs optimize performance.\n",
        "The explain() function in MongoDB is an essential performance tuning tool. It helps you understand how queries are processed, whether indexes are being used effectively, and where bottlenecks may exist — enabling you to write faster and more efficient database queries.\n"
      ],
      "metadata": {
        "id": "A4bxlMwpykEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12.How does MongoDB handle schema validation.\n",
        "MongoDB handles schema validation using a feature called JSON Schema validation, which allows us to define rules for the structure of documents in a collection. While MongoDB is schema-less by default (meaning documents in a collection can have any shape), you can enforce structure and constraints by specifying a validation schema.\n",
        "MongoDB uses $jsonSchema as part of the collection options to define a validation rule. This schema is based on the JSON Schema standard."
      ],
      "metadata": {
        "id": "6byR946cd0Vv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13.What is the difference between a primary and a secondary node in a replica set?\n",
        "The primary and secondary nodes in a MongoDB replica set differ in terms of their roles in data management, consistency, and availability.\n",
        "primary Node:\n",
        "Acts as the main authoritative source of data.\n",
        "Handles all write operations and coordinates updates.\n",
        "Ensures the initial propagation of changes to secondaries.\n",
        "Generates an oplog (operations log) recording all write actions.\n",
        "This log becomes the source of truth for replication.\n",
        "Accepts all writes; ensures strong data consistency.\n",
        "Default target for read operations.\n",
        "Secondary Node:\n",
        "Functions as a replica or backup.\n",
        "Maintains a copy of the primary’s data via replication.\n",
        "Ensures data redundancy and fault tolerance.\n",
        "Continuously reads from the primary’s oplog.\n",
        "Applies the operations locally to keep in sync with the primary.\n",
        "Does not accept writes (except under special configurations).\n",
        "Can serve read-only queries if read preference allows.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jwx6RYFDevS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14.What security mechanisms does MongoDB provide for data protection?\n",
        "MongoDB provides a comprehensive set of security mechanisms to protect data at various levels—network, authentication, authorization, encryption, and auditing.\n",
        "1.Authentication(Ensures only verified users can access the database)\tSCRAM, x.509, LDAP, Kerberos, IAM (Atlas)\n",
        "2.Authorization(Controls what authenticated users are allowed to do)Role-based access control (RBAC)\n",
        "3.Encryption(at rest)(Protects data at rest and in transit.)\tAES-256, KMIP, Cloud KMS (Atlas)\n",
        "4.Encryption(in transit)\tTLS/SSL\n",
        "5.Auditing(Tracks database operations, user actions, and access patterns.)\tEnterprise auditing logs\n",
        "6.Network Security(Restricts access to trusted IP addresses.)\tIP whitelisting, firewalls, VPC peering, bindIP"
      ],
      "metadata": {
        "id": "s0aj4MWky30I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#15.Explain the concept of embedded documents and when they should be used.\n",
        "An embedded document is a document nested inside another document in MongoDB. Instead of storing related data in a separate collection and linking them via references (like in relational databases), MongoDB allows you to store related data together in the same document.\n",
        "Embedded documents are used when\n",
        "1.Data is tightly coupled (one-to-few relationship)\n",
        "→ E.g., a user and their profile or settings.\n",
        "2.To avoid joins\n",
        "→ MongoDB doesn’t support traditional joins efficiently; embedding allows atomic access to all related data.\n",
        "3.frequently access the parent and child data together\n",
        "→ Embedding reduces the number of read operations.\n",
        "4.The embedded data will not grow unbounded\n",
        "→ MongoDB documents have a size limit of 16MB.\n",
        "5.Data is mostly read together and updated as a whole\n",
        "→ Embedding simplifies atomic updates and improves performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "emuZ3ZU80Ias"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#16.What is the purpose of MongoDB’s $lookup stage in aggregation?\n",
        "\n",
        "The $lookup stage in MongoDB’s aggregation pipeline is used to perform a left outer join between documents from two collections—similar to SQL joins.\n",
        "It Combines documents from a main (local) collection with matching documents from a foreign collection.\n",
        "Adds an array field to each input document containing the matched documents.\n",
        "It Fetch related data from another collection (e.g., order details + customer info).Mimic SQL-style joins in MongoDB.Create denormalized views for reporting or analytics."
      ],
      "metadata": {
        "id": "CNJk0cu_1Ao3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#17.What are some common use cases for MongoDB.\n",
        "MongoDB is a flexible, document-oriented NoSQL database designed for scalability, high performance, and ease of development. Its schema-less and JSON-like (BSON) data model makes it ideal for many modern applications.\n",
        "Some use cases of MongoDB.\n",
        "1.CMS & Media\t(Flexible document model)\n",
        "2.E-Commerce\t(Varying product structures)\n",
        "3.User Management\t(Dynamic user profiles)\n",
        "4.Analytics & Dashboards\t(Aggregation pipeline, real-time writes)\n",
        "5.Mobile & Web Apps\t(JSON format, offline sync, scalability)\n",
        "6.Geospatial Data\t(Native geo queries and indexes)\n",
        "7.IoT & Sensor Data\t(Time-series and fast ingestion)\n",
        "8.Logs and Events\t(High write throughput, flexible schema)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ASFySSCp6-PV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#18.What are the advantages of using MongoDB for horizontal scaling.\n",
        "MongoDB is designed for horizontal scaling using a technique called sharding, which allows it to handle massive volumes of data and traffic by distributing the load across multiple machines.\n",
        "Here are some Advantages of using MongoDB for horizontal scaling.\n",
        "1.Data Distribution\t(Shards divide data by key and distribute across nodes)\n",
        "2.Performance\t(Parallel processing increases throughput and reduces latenc)\n",
        "3.Scalability\t(Add nodes without downtime)\n",
        "4.Fault Tolerance\t(Shards are replica sets with automatic failover)\n",
        "5.Load Balancing\t(MongoDB balances data across shards automatically)\n",
        "6.Geographic Reach\t(Shards can span data centers/regions)"
      ],
      "metadata": {
        "id": "LACZwEWD8Si-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#19. How do MongoDB transactions differ from SQL transactions\n",
        "MongoDB transactions differ from SQL transactions mainly in scope and usage:\n",
        "MongoDB is document-based and supports atomic operations on single documents by default. Multi-document transactions are supported (since v4.0), but can impact performance and are best used sparingly.\n",
        "SQL databases are table-based and designed for frequent, complex transactions across multiple tables with mature, built-in ACID support."
      ],
      "metadata": {
        "id": "71WSVZ5G9VU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#20. What are the main differences between capped collections and regular collections?\n",
        "Capped collections are fixed-size, auto-overwriting, and maintain insertion order—ideal for logs and real-time data.\n",
        "Regular collections are flexible, grow dynamically, support full CRUD, and allow document deletion and indexing"
      ],
      "metadata": {
        "id": "ZfvYjqKA9uiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#21.What is the purpose of the $match stage in MongoDB’s aggregation pipeline?\n",
        "\n",
        "The $match stage in MongoDB’s aggregation pipeline is used to filter documents based on specified criteria—similar to the WHERE clause in SQL.\n",
        "Purpose:\n",
        "To narrow down documents early in the pipeline, improving performance by processing only relevant data.\n",
        "Eg:{ $match: { status: \"active\" } }\n",
        "only documents with active status will pass to the next stage."
      ],
      "metadata": {
        "id": "1iGouM3o-BON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#22.How can you secure access to a MongoDB database?\n",
        "To secure a MongoDB database, you can implement the following key measures:\n",
        "1.Enable authentication\n",
        "2.Use RBAC for least privilege access\n",
        "3.Use TLS/SSL encryption\n",
        "4.Enable encryption at rest\n",
        "5.Restrict network access\n",
        "6.Disable unused services\n",
        "7.Monitor with audit logs\n",
        "8.Keep MongoDB version updated"
      ],
      "metadata": {
        "id": "VC-oYO55-boJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#23.What is MongoDB’s WiredTiger storage engine, and why is it important?\n",
        "WiredTiger is the default storage engine in MongoDB since version 3.2. It is responsible for how data is stored, managed, and accessed on disk.\n",
        "It is important because of\n",
        "1.Performance: Faster reads/writes with fine-grained concurrency\n",
        "2.Efficiency: Lower storage use via compression\n",
        "3.Stability: Durable and crash-safe architecture\n",
        "4.Security: Supports encrypted storage in secure deployments\n"
      ],
      "metadata": {
        "id": "JZZax4Yl_AFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "2RvH9aj2AdkM",
        "outputId": "45fb5f6c-b0ef-4ec5-8e40-58b746041f50"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f5fe6044-cb66-43e6-a0da-1b76147460cf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f5fe6044-cb66-43e6-a0da-1b76147460cf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving superstore.csv to superstore.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.Write a Python script to load the Superstore dataset from a CSV file into MongoDB<\n",
        "!pip install pymongo\n",
        "import pandas as pd\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# 1. Load CSV using pandas\n",
        "csv_file_path = \"superstore.csv\"\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# 2. Convert DataFrame to dictionary records\n",
        "records = df.to_dict(orient='records')\n",
        "\n",
        "# 3. Connect to MongoDB\n",
        "client = MongoClient(\"mongodb://localhost/portname\")\n",
        "db = client[\"superstore_db\"]\n",
        "collection = db[\"orders\"]\n",
        "\n",
        "# 4. Insert records into MongoDB\n",
        "collection.insert_many(records)\n",
        "\n",
        "print(f\"Inserted {len(records)} documents into the 'orders' collection.\")\n"
      ],
      "metadata": {
        "id": "T06Fdp0oCyc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Retrieve and print all documents from the Orders collection.\n",
        "from pymongo import MongoClient\n",
        "# Connect to MongoDB\n",
        "client = MongoClient(\"mongodb://hostname/portname\")\n",
        "db = client[\"superstore_db\"]\n",
        "collection = db[\"orders\"]\n",
        "# Retrieve and print all documents\n",
        "all_orders = collection.find()\n",
        "for order in all_orders:\n",
        "    print(order)\n"
      ],
      "metadata": {
        "id": "NeqAFTIiDMET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Count and display the total number of documents in the Orders collection.\n",
        "from pymongo import MongoClient\n",
        "# Connect to MongoDB\n",
        "client = MongoClient(\"mongodb://hostname/portname\")\n",
        "db = client[\"superstore_db\"]\n",
        "collection = db[\"orders\"]\n",
        "# Count documents\n",
        "total_documents = collection.count_documents({})\n",
        "# Display the count\n",
        "print(f\"Total number of documents in 'orders' collection: {total_documents}\")\n"
      ],
      "metadata": {
        "id": "bt8XWQqlDdUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.Write a query to fetch all orders from the \"West\" region.\n",
        "from pymongo import MongoClient\n",
        "# Connect to MongoDB\n",
        "client = MongoClient(\"mongodb://hostname/portname\")\n",
        "db = client[\"superstore_db\"]\n",
        "collection = db[\"orders\"]\n",
        "# Query: Fetch orders where Region is \"West\"\n",
        "west_orders = collection.find({\"Region\": \"West\"})\n",
        "# Print results\n",
        "for order in west_orders:\n",
        "    print(order)\n"
      ],
      "metadata": {
        "id": "b4ztoZcWD4NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5.Write a query to find orders where Sales is greater than 500.\n",
        "from pymongo import MongoClient\n",
        "# Connect to MongoDB\n",
        "client = MongoClient(\"mongodb://hostname/portname\")\n",
        "db = client[\"superstore_db\"]\n",
        "collection = db[\"orders\"]\n",
        "# Query: Sales > 500\n",
        "high_sales_orders = collection.find({ \"Sales\": { \"$gt\": 500 } })\n",
        "# Print matching orders\n",
        "for order in high_sales_orders:\n",
        "    print(order)\n"
      ],
      "metadata": {
        "id": "I75qSwB7EQJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6.Fetch the top 3 orders with the highest Profit.\n",
        "from pymongo import MongoClient\n",
        "# Connect to MongoDB\n",
        "client = MongoClient(\"mongodb://localhost:27017/\")\n",
        "db = client[\"superstore_db\"]\n",
        "collection = db[\"orders\"]\n",
        "# Query: Top 3 orders by Profit\n",
        "top_profit_orders = collection.find().sort(\"Profit\", -1).limit(3)\n",
        "# Print results\n",
        "for order in top_profit_orders:\n",
        "    print(order)\n"
      ],
      "metadata": {
        "id": "c_CKg2J1E6tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7.Update all orders with Ship Mode as \"First Class\" to \"Premium Class.\n",
        "from pymongo import MongoClient\n",
        "# Connect to MongoDB\n",
        "client = MongoClient(\"mongodb://localhost:27017/\")\n",
        "db = client[\"superstore_db\"]\n",
        "collection = db[\"orders\"]\n",
        "# Update all \"First Class\" Ship Mode to \"Premium Class\"\n",
        "result = collection.update_many(\n",
        "    { \"Ship Mode\": \"First Class\" },\n",
        "    { \"$set\": { \"Ship Mode\": \"Premium Class\" } }\n",
        ")\n",
        "# Print the number of documents updated\n",
        "print(f\"Updated {result.modified_count} documents.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "W0QIUIQIFFwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Delete all orders where Sales is less than 50.\n",
        "from pymongo import MongoClient\n",
        "# Connect to MongoDB\n",
        "client = MongoClient(\"mongodb://localhost:27017/\")\n",
        "db = client[\"superstore_db\"]\n",
        "collection = db[\"orders\"]\n",
        "# Delete all orders where Sales < 50\n",
        "result = collection.delete_many({ \"Sales\": { \"$lt\": 50 } })\n",
        "# Print number of documents deleted\n",
        "print(f\"Deleted {result.deleted_count} documents.\")\n"
      ],
      "metadata": {
        "id": "WT7Yc1PfFUgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9.Use aggregation to group orders by Region and calculate total sales per region.\n",
        "from pymongo import MongoClient\n",
        "# Connect to MongoDB\n",
        "client = MongoClient(\"mongodb://localhost:27017/\")\n",
        "db = client[\"superstore_db\"]\n",
        "collection = db[\"orders\"]\n",
        "# Aggregation: Total sales per region\n",
        "pipeline = [\n",
        "    {\n",
        "        \"$group\": {\n",
        "            \"_id\": \"$Region\",\n",
        "            \"totalSales\": { \"$sum\": \"$Sales\" }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "results = collection.aggregate(pipeline)\n",
        "# Print the results\n",
        "for result in results:\n",
        "    print(f\"Region: {result['_id']}, Total Sales: {result['totalSales']}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Pjmj7zZ4FeRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10.Fetch all distinct values for Ship Mode from the collection.\n",
        "from pymongo import MongoClient\n",
        "# Connect to MongoDB\n",
        "client = MongoClient(\"mongodb://localhost:27017/\")\n",
        "db = client[\"superstore_db\"]\n",
        "collection = db[\"orders\"]\n",
        "# Fetch distinct Ship Mode values\n",
        "ship_modes = collection.distinct(\"Ship Mode\")\n",
        "# Print results\n",
        "print(\"Distinct Ship Modes:\", ship_modes)\n"
      ],
      "metadata": {
        "id": "aEwx0OdaFqS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11.Count the number of orders for each category.\n",
        "from pymongo import MongoClient\n",
        "# Connect to MongoDB\n",
        "client = MongoClient(\"mongodb://localhost:27017/\")\n",
        "db = client[\"superstore_db\"]\n",
        "collection = db[\"orders\"]\n",
        "# Aggregation: Count orders per Category\n",
        "pipeline = [\n",
        "    {\n",
        "        \"$group\": {\n",
        "            \"_id\": \"$Category\",\n",
        "            \"orderCount\": { \"$sum\": 1 }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "results = collection.aggregate(pipeline)\n",
        "# Print the results\n",
        "for result in results:\n",
        "    print(f\"Category: {result['_id']}, Order Count: {result['orderCount']}\")\n"
      ],
      "metadata": {
        "id": "z-aObJDUF3j0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}